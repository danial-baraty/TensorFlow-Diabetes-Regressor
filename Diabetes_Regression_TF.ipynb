{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b709d8-f9f4-4a87-a23c-4e8479dc22d0",
   "metadata": {},
   "source": [
    "# The Diabetes Dataset (Scikit-learn)\n",
    "\n",
    "This is a classic **Regression Problem**—meaning our goal is to predict a **continuous number**, not a category (like 'Yes/No').\n",
    "\n",
    "🎯 What We're Predicting (The Target)\n",
    "\n",
    "The model predicts a **quantitative measure of disease progression** in a patient **one year** after their initial check-up.\n",
    "\n",
    "* **Range:** The values are between about 25 and 346.\n",
    "\n",
    "📊 The 10 Input Features (X)\n",
    "\n",
    "We use 10 baseline measurements to make the prediction:\n",
    "\n",
    "* **Examples:** Age, Sex, BMI (Body Mass Index), BP (Average Blood Pressure), and various **Serum Measurements**.\n",
    "\n",
    "**Note:** All 10 features have already been **scaled (normalized)** for easier model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a2249-0218-4e4e-bcc8-835c8213dff7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b89e6ba-fb05-4497-8b8d-668fbcd6c6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.052606</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.009439</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.050717</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.119340</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>-0.061809</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.015568</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.039618</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.048351</td>\n",
       "      <td>-0.033255</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.101640</td>\n",
       "      <td>-0.067351</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.057941</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.067615</td>\n",
       "      <td>-0.068328</td>\n",
       "      <td>-0.054446</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>-0.083920</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.041840</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.128521</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>-0.033216</td>\n",
       "      <td>-0.032629</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>-0.050783</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.056239</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.021817</td>\n",
       "      <td>0.056301</td>\n",
       "      <td>-0.007073</td>\n",
       "      <td>0.018101</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.023647</td>\n",
       "      <td>0.023775</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.074533</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.043373</td>\n",
       "      <td>-0.033213</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.063367</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.027129</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.016412</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.035760</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.021395</td>\n",
       "      <td>-0.034215</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.014272</td>\n",
       "      <td>0.042529</td>\n",
       "      <td>-0.030464</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.033246</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "371  0.052606  0.050680 -0.009439  0.049415  0.050717 -0.019163 -0.013948   \n",
       "402  0.110727  0.050680 -0.033151 -0.022885 -0.004321  0.020293 -0.061809   \n",
       "220  0.023546  0.050680 -0.039618 -0.005670 -0.048351 -0.033255  0.011824   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "78   0.005383 -0.044642 -0.057941 -0.022885 -0.067615 -0.068328 -0.054446   \n",
       "145 -0.041840 -0.044642  0.128521  0.063187 -0.033216 -0.032629  0.011824   \n",
       "342  0.056239  0.050680  0.021817  0.056301 -0.007073  0.018101 -0.032356   \n",
       "134 -0.074533 -0.044642  0.043373 -0.033213  0.012191  0.000252  0.063367   \n",
       "156 -0.016412 -0.044642 -0.010517  0.001215 -0.037344 -0.035760  0.011824   \n",
       "140  0.041708  0.050680  0.014272  0.042529 -0.030464 -0.001314 -0.043401   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "371  0.034309  0.119340 -0.017646   197.0  \n",
       "402  0.071210  0.015568  0.044485   168.0  \n",
       "220 -0.039493 -0.101640 -0.067351    78.0  \n",
       "439 -0.011080 -0.046883  0.015491   132.0  \n",
       "78  -0.002592  0.042897 -0.083920   252.0  \n",
       "145 -0.039493 -0.015999 -0.050783   259.0  \n",
       "342 -0.002592 -0.023647  0.023775   178.0  \n",
       "134 -0.039493 -0.027129 -0.046641   103.0  \n",
       "156 -0.039493 -0.021395 -0.034215    25.0  \n",
       "140 -0.002592 -0.033246  0.015491   118.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load diabetes dataset\n",
    "data = load_diabetes()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe63079-2d01-4e5b-94a8-a9b9ed27a075",
   "metadata": {},
   "source": [
    "# Split data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e48c074d-3674-4c15-b14c-c05cf0381938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b628c-e8a6-4692-af8e-f536a860807e",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80ded341-07c4-4cc8-b2ca-ac4d177465ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential # A linear stack of layers\n",
    "from tensorflow.keras.layers import Dense # Fully connected layer where each neuron connects to all neurons in previous layer\n",
    "from tensorflow.keras.optimizers import Adam # Import optimizer if we want to specify how the model updates weights\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer: 64 neurons, ReLU activation, input dimension equals number of features\n",
    "    Dense(32, activation='relu'),  # Second hidden layer: 32 neurons, ReLU activation\n",
    "    Dense(1)  # Output layer: 1 neuron for regression (linear output, no activation)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af528d4a-00a7-4bde-a123-8ed5646bf7d3",
   "metadata": {},
   "source": [
    "# Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2229a3f0-b321-4178-9a34-5c4169ad4093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Updates weights using gradients\n",
    "    loss='mse',                            # Loss: what model tries to minimize \n",
    "    metrics=['mae']                        # Metric: what we track for monitoring\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352277a-8bbe-40f6-a449-2b59e04dd9f8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5c6ce6d-a2f5-4791-b38c-7ad1f5813f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 31525.1055 - mae: 158.6667 - val_loss: 22479.9375 - val_mae: 134.0734\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 31506.2734 - mae: 158.6090 - val_loss: 22463.6016 - val_mae: 134.0139\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 31482.9961 - mae: 158.5393 - val_loss: 22442.5762 - val_mae: 133.9377\n",
      "Epoch 4/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 31453.3086 - mae: 158.4492 - val_loss: 22414.8105 - val_mae: 133.8377\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 31413.6270 - mae: 158.3299 - val_loss: 22378.3477 - val_mae: 133.7062\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 31361.5859 - mae: 158.1728 - val_loss: 22330.1094 - val_mae: 133.5321\n",
      "Epoch 7/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 31292.7793 - mae: 157.9661 - val_loss: 22267.2051 - val_mae: 133.3050\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 31201.5781 - mae: 157.6978 - val_loss: 22186.5488 - val_mae: 133.0131\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 31090.0996 - mae: 157.3511 - val_loss: 22082.6172 - val_mae: 132.6361\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 30941.0078 - mae: 156.9144 - val_loss: 21955.3008 - val_mae: 132.1725\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 30764.7090 - mae: 156.3761 - val_loss: 21798.0137 - val_mae: 131.5981\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 30541.7695 - mae: 155.7137 - val_loss: 21609.5000 - val_mae: 130.9065\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 30281.2988 - mae: 154.9143 - val_loss: 21382.9219 - val_mae: 130.0713\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 29973.6914 - mae: 153.9649 - val_loss: 21116.1914 - val_mae: 129.0829\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 29607.2207 - mae: 152.8453 - val_loss: 20809.6484 - val_mae: 127.9374\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 29193.1602 - mae: 151.5497 - val_loss: 20458.5938 - val_mae: 126.6149\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 28728.0273 - mae: 150.0713 - val_loss: 20061.1367 - val_mae: 125.1012\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 28179.4609 - mae: 148.3804 - val_loss: 19626.6582 - val_mae: 123.4233\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 27584.2305 - mae: 146.4971 - val_loss: 19143.5898 - val_mae: 121.5314\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26944.1641 - mae: 144.3889 - val_loss: 18604.8457 - val_mae: 119.3896\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 26212.7168 - mae: 142.0278 - val_loss: 18027.3555 - val_mae: 117.0500\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 25448.2910 - mae: 139.4244 - val_loss: 17398.3301 - val_mae: 114.4503\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 24602.1855 - mae: 136.5564 - val_loss: 16731.6816 - val_mae: 111.6287\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 23700.4102 - mae: 133.4467 - val_loss: 16026.2676 - val_mae: 108.5667\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22742.7793 - mae: 130.0734 - val_loss: 15287.3994 - val_mae: 105.2642\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21743.6914 - mae: 126.4960 - val_loss: 14515.3643 - val_mae: 101.7044\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20718.9668 - mae: 122.5797 - val_loss: 13710.4980 - val_mae: 97.8583\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19624.0254 - mae: 118.4393 - val_loss: 12896.0967 - val_mae: 93.8060\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 18540.4902 - mae: 114.0279 - val_loss: 12058.6953 - val_mae: 89.4538\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 17402.0664 - mae: 109.3891 - val_loss: 11229.8701 - val_mae: 85.0485\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16260.2500 - mae: 104.6526 - val_loss: 10413.8682 - val_mae: 80.8105\n",
      "Epoch 32/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 15149.2715 - mae: 99.9115 - val_loss: 9601.9229 - val_mae: 76.5158\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14031.3008 - mae: 95.2029 - val_loss: 8816.0000 - val_mae: 72.2324\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 12939.9766 - mae: 90.6278 - val_loss: 8062.7393 - val_mae: 67.9959\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 11873.5537 - mae: 86.3332 - val_loss: 7358.8149 - val_mae: 63.9259\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10866.0742 - mae: 82.0860 - val_loss: 6705.9238 - val_mae: 60.2136\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9926.4170 - mae: 77.8788 - val_loss: 6101.9849 - val_mae: 57.0421\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9053.5059 - mae: 73.9009 - val_loss: 5557.3027 - val_mae: 54.3881\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 8246.9902 - mae: 70.3637 - val_loss: 5080.0635 - val_mae: 52.2039\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 7515.3394 - mae: 67.0833 - val_loss: 4676.7749 - val_mae: 50.7321\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6856.4639 - mae: 64.5376 - val_loss: 4346.4155 - val_mae: 49.6155\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 6328.6875 - mae: 62.3689 - val_loss: 4066.6523 - val_mae: 48.6969\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 5825.5640 - mae: 60.3055 - val_loss: 3857.6821 - val_mae: 48.1477\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5419.2393 - mae: 58.8284 - val_loss: 3702.5916 - val_mae: 47.7940\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 5085.5845 - mae: 57.6362 - val_loss: 3592.4866 - val_mae: 47.7913\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4813.1152 - mae: 56.5706 - val_loss: 3522.1462 - val_mae: 48.0653\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4584.8076 - mae: 55.7033 - val_loss: 3482.5718 - val_mae: 48.3780\n",
      "Epoch 48/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4415.5649 - mae: 55.0088 - val_loss: 3464.1257 - val_mae: 48.7543\n",
      "Epoch 49/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 4282.3535 - mae: 54.5751 - val_loss: 3462.3022 - val_mae: 49.2329\n",
      "Epoch 50/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4180.3896 - mae: 54.1090 - val_loss: 3473.0122 - val_mae: 49.6685\n",
      "Epoch 51/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4088.7283 - mae: 53.6621 - val_loss: 3488.2012 - val_mae: 50.0693\n",
      "Epoch 52/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 4022.6353 - mae: 53.3063 - val_loss: 3505.8821 - val_mae: 50.4085\n",
      "Epoch 53/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3970.3352 - mae: 53.0117 - val_loss: 3524.6580 - val_mae: 50.7218\n",
      "Epoch 54/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3929.5876 - mae: 52.7616 - val_loss: 3543.4836 - val_mae: 50.9919\n",
      "Epoch 55/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3894.6086 - mae: 52.5916 - val_loss: 3560.5815 - val_mae: 51.2048\n",
      "Epoch 56/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3865.2935 - mae: 52.4207 - val_loss: 3574.7034 - val_mae: 51.3651\n",
      "Epoch 57/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3838.7690 - mae: 52.2757 - val_loss: 3586.9036 - val_mae: 51.4933\n",
      "Epoch 58/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3816.6028 - mae: 52.1649 - val_loss: 3596.7051 - val_mae: 51.6061\n",
      "Epoch 59/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 3791.1653 - mae: 51.9681 - val_loss: 3594.3606 - val_mae: 51.5870\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Convert to float32 for TensorFlow\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3118788-faca-4c68-8901-97f2d6afe76b",
   "metadata": {},
   "source": [
    "**After each Batch (forward pass):**\n",
    "\n",
    "* The model makes predictions on the batch.\n",
    "* Loss is computed on the same batch.\n",
    "* Weights are updated via backpropagation → the model learns.\n",
    "\n",
    "**After each Epoch (validation check):**\n",
    "\n",
    "* The model makes predictions on the entire validation set.\n",
    "* `val_loss` and metrics are computed.\n",
    "* Weights are **not** updated.\n",
    "* Only check whether EarlyStopping should continue training or stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162954c-1721-4179-800c-77d52d4c24ea",
   "metadata": {},
   "source": [
    "**ٍExample**\n",
    "\n",
    "| Epoch | val_loss | Best val_loss | Improved? |\n",
    "| ----- | -------- | ------------- | --------- |\n",
    "| 1     | 0.50     | inf           | Yes       |\n",
    "| 2     | 0.45     | 0.50          | Yes       |\n",
    "| 3     | 0.47     | 0.45          | No        |\n",
    "| 4     | 0.44     | 0.45          | Yes       |\n",
    "\n",
    "EarlyStopping with `patience=10` stops training if val_loss does not improve for 10 epochs in a row.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243bb853-d2fa-440c-94ea-4f8409c19877",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "The target values (`Rating`) were highly skewed, so we applied Yeo-Johnson transformation to normalize them.  \n",
    "Before evaluating the model on the test set, we **inverse transform the predictions and the true target values** back to the original scale to compute meaningful metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bbddde0-e0e0-49b8-958a-b47c607e0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "R²: 0.3338\n",
      "RMSE: 59.4111\n",
      "MAE: 49.2895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Ensure test features and target are numeric\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)\n",
    "\n",
    "# Predict on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate metrics directly on  values\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"R²:\", round(r2, 4))\n",
    "print(\"RMSE:\", round(rmse, 4))\n",
    "print(\"MAE:\", round(mae, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbbe8-48bb-4549-9ea8-deac9273b969",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The model underfits and needs improvement.  \n",
    "\n",
    "- Increase network capacity: more layers, more neurons, or different architectures.  \n",
    "- Feature engineering: create new features to better explain the target.  \n",
    "- Hyperparameter tuning: adjust learning rate, batch size, or epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39aad9-9c34-4021-8f35-8464a0e19f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
